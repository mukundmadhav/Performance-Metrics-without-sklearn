{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"5_a.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10100, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    10000\n",
       "0.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = df['y'].value_counts()\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Compute Confusion Matrix\n",
    "##  2. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "def calculate_tp_fp_tn_fn(data):\n",
    "    TP , TN, FP, FN = 0 , 0 , 0 ,0\n",
    "    y_predict_new = []\n",
    "    for i in data['proba']:\n",
    "        if (i < 0.5):\n",
    "            y_predict_new.append(0.0)\n",
    "        else :\n",
    "            y_predict_new.append(1.0)\n",
    "            \n",
    "    data['y_predict']=y_predict_new\n",
    "    \n",
    "    TN = ((data['y']==0.0) & (data['y_predict']==0.0)).sum()\n",
    "    TP = ((data['y']==1.0) & (data['y_predict']==1.0)).sum()\n",
    "    FP = ((data['y']==0.0) & (data['y_predict']==1.0)).sum()\n",
    "    FN = ((data['y']==1.0) & (data['y_predict']==0.0)).sum()\n",
    "    \n",
    "    return (TP,TN,FN,FP)\n",
    "\n",
    "def compute_f1_score(TP,TN,FN,FP):\n",
    "    \n",
    "    precision = TP/(FP+TP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1_Score = (2* (precision*Recall)/(precision+Recall))\n",
    "    \n",
    "    return (precision,Recall,F1_Score)\n",
    "\n",
    "def confusion_matrix(TP,TN,FP,FN):\n",
    "    return (np.array([[TN , FP],[FN,TP]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix: [  0 100] \n",
      " \t\t [    0 10000]\n",
      "F1_Score: 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "TP,TN,FN,FP = calculate_tp_fp_tn_fn(df)\n",
    "precision,Recall,F1_Score = compute_f1_score(TP,TN,FN,FP)\n",
    "a =confusion_matrix(TP,TN,FP,FN)\n",
    "\n",
    "print('Confusion_Matrix:',a[0],'\\n','\\t'*2,a[1])\n",
    "print('F1_Score:', F1_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10100/10100 [01:46<00:00, 94.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.48829900000000004\n",
      "TP : 10000\n",
      "TN : 0\n",
      "FP : 100\n",
      "FN : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tpr=[]\n",
    "fpr=[]\n",
    "data = pd.read_csv('5_a.csv')\n",
    "uniq_data = list(data.proba.unique())\n",
    "uniq_data.sort(reverse = True)\n",
    "\n",
    "\n",
    "for tow in tqdm(uniq_data):\n",
    "    y_predict_new = []\n",
    "    for i in data['proba']:\n",
    "        if (i <  tow):\n",
    "            y_predict_new.append(0)\n",
    "        else :\n",
    "            y_predict_new.append(1)\n",
    "           \n",
    "    data['y_predict']=y_predict_new\n",
    "   \n",
    "    TN = (((data['y'])==0)& ((data['y_predict'])==0)).sum()\n",
    "    TP = (((data['y'])==1) & ((data['y_predict'])==1)).sum()\n",
    "    FP = (((data['y'])==0) & ((data['y_predict'])==1)).sum()\n",
    "    FN = (((data['y'])==1) & ((data['y_predict'])==0)).sum()\n",
    "       \n",
    "    tpr.append(TP/(TP+FN))\n",
    "    fpr.append(FP/(FP+TN))\n",
    "\n",
    "x = sorted(tpr)\n",
    "y = sorted(fpr)\n",
    "auc = np.trapz(x,y)\n",
    "   \n",
    "print('AUC Score :',auc)\n",
    "print('TP :', TP)\n",
    "print('TN :', TN)\n",
    "print('FP :', FP)\n",
    "print('FN :', FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "def check_equality(df):\n",
    "    p=0\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        if df['y'][i]==df['y_predict'][i]:\n",
    "            p+=1\n",
    "    return p\n",
    "total_Positive = check_equality(df)\n",
    "q = len((df))\n",
    "auc_score = total_Positive/q\n",
    "print('Accuracy Score:', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  0.0  0.281035\n",
       "1  0.0  0.465152\n",
       "2  0.0  0.352793\n",
       "3  0.0  0.157818\n",
       "4  0.0  0.276648"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code\n",
    "df_b = pd.read_csv(\"5_b.csv\")\n",
    "df_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10100, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10000\n",
       "1.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = df_b['y'].value_counts()\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Compute Confusion Matrix\n",
    "##  2. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "def calculate_tp_fp_tn_fn(data):\n",
    "    TP , TN, FP, FN = 0 , 0 , 0 ,0\n",
    "    y_predict_new = []\n",
    "    for i in data['proba']:\n",
    "        if (i < 0.5):\n",
    "            y_predict_new.append(0.0)\n",
    "        else :\n",
    "            y_predict_new.append(1.0)\n",
    "            \n",
    "    data['y_predict']=y_predict_new\n",
    "    \n",
    "    TN = ((data['y']==0.0) & (data['y_predict']==0.0)).sum()\n",
    "    TP = ((data['y']==1.0) & (data['y_predict']==1.0)).sum()\n",
    "    FP = ((data['y']==0.0) & (data['y_predict']==1.0)).sum()\n",
    "    FN = ((data['y']==1.0) & (data['y_predict']==0.0)).sum()\n",
    "    \n",
    "    return (TP,TN,FN,FP)\n",
    "\n",
    "def compute_f1_score(TP,TN,FN,FP):\n",
    "    \n",
    "    precision = TP/(FP+TP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1_Score = (2* (precision*Recall)/(precision+Recall))\n",
    "    \n",
    "    return (precision,Recall,F1_Score)\n",
    "\n",
    "def confusion_matrix(TP,TN,FP,FN):\n",
    "    return (np.array([[TN , FP],[FN,TP]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix: [9761  239] \n",
      " \t\t [45 55]\n",
      "F1_Score: 0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "TP,TN,FN,FP = calculate_tp_fp_tn_fn(df_b)\n",
    "precision,Recall,F1_Score = compute_f1_score(TP,TN,FN,FP)\n",
    "a =confusion_matrix(TP,TN,FP,FN)\n",
    "\n",
    "print('Confusion_Matrix:',a[0],'\\n','\\t'*2,a[1])\n",
    "print('F1_Score:', F1_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10100/10100 [01:56<00:00, 86.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.9377570000000001\n",
      "TP : 100\n",
      "TN : 0\n",
      "FP : 10000\n",
      "FN : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tpr=[]\n",
    "fpr=[]\n",
    "data1 = pd.read_csv('5_b.csv')\n",
    "uniq_data = list(data1.proba.unique())\n",
    "uniq_data.sort(reverse = True)\n",
    "\n",
    "\n",
    "for tow in tqdm(uniq_data):\n",
    "    y_predict_new = []\n",
    "    for i in data1['proba']:\n",
    "        if (i <  tow):\n",
    "            y_predict_new.append(0)\n",
    "        else :\n",
    "            y_predict_new.append(1)\n",
    "           \n",
    "    data1['y_predict']=y_predict_new\n",
    "   \n",
    "    TN = (((data1['y'])==0)& ((data1['y_predict'])==0)).sum()\n",
    "    TP = (((data1['y'])==1) & ((data1['y_predict'])==1)).sum()\n",
    "    FP = (((data1['y'])==0) & ((data1['y_predict'])==1)).sum()\n",
    "    FN = (((data1['y'])==1) & ((data1['y_predict'])==0)).sum()\n",
    "       \n",
    "    tpr.append(TP/(TP+FN))\n",
    "    fpr.append(FP/(FP+TN))\n",
    "\n",
    "x = sorted(tpr)\n",
    "y = sorted(fpr)\n",
    "auc = np.trapz(x,y)\n",
    "   \n",
    "print('AUC Score :',auc)\n",
    "print('TP :', TP)\n",
    "print('TN :', TN)\n",
    "print('FP :', FP)\n",
    "print('FN :', FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "def check_equality(df):\n",
    "    p=0\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        if df_b['y'][i]==df_b['y_predict'][i]:\n",
    "            p+=1\n",
    "    return p\n",
    "total_Positive = check_equality(df_b)\n",
    "q = len((df_b))\n",
    "auc_score = total_Positive/q\n",
    "print('Accuracy Score:', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the best threshold which gives lowest values of metric A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2852/2852 [00:11<00:00, 253.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Value:  0.22987164436159915\n",
      "Minimum value of A: 141000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "from tqdm import tqdm\n",
    "dict = {}\n",
    "data_c = pd.read_csv('5_c.csv')\n",
    "data_c.head()\n",
    "sorted_data = data_c.sort_values(by='prob', ascending = True)\n",
    "uniq_prob = list(data_c.prob.unique())\n",
    "uniq_prob.sort(reverse = True)\n",
    "A = []\n",
    "for threshold in tqdm(data_c['prob']):\n",
    "   \n",
    "    y_predict=[]\n",
    "    for value in data_c['prob']:\n",
    "        if (value <= threshold):\n",
    "            y_predict.append(0)\n",
    "            \n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "    data_c['y_predict1'] = y_predict\n",
    "    \n",
    "\n",
    "    FP = ((data_c['y']==0) & (data_c['y_predict1']==1)).sum()\n",
    "    FN = ((data_c['y']==1) & (data_c['y_predict1']==0)).sum()\n",
    "    A.append((500 * FN) + (100 * FP))\n",
    "    \n",
    "b = pd.Series(A)\n",
    "min_index = min(b)\n",
    "threshold_index = A.index(min_index)\n",
    "print('Threshold Value: ',sorted_data['prob'][threshold_index])\n",
    "print('Minimum value of A:', min_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error:  177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "\n",
    "df_5d= pd.read_csv('5_d.csv')\n",
    "\n",
    "MSE = 0\n",
    "n =len(df_5d)\n",
    "for i in range(len(df_5d)):\n",
    "    MSE = MSE + (df_5d['y'][i] - df_5d['pred'][i])**2\n",
    "MSE = 1/(n)*MSE\n",
    "print(\"Mean Square Error: \", MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Modified - MAPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified - MAPE : 0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "df_5d = pd.read_csv('5_d.csv')\n",
    "ei =0\n",
    "ai=0\n",
    "for i in range(len(df_5d)):\n",
    "    ei+= abs(df_5d['y'][i] - df_5d['pred'][i])\n",
    "    ai+= (df_5d['y'][i])\n",
    "MAPE = ei / ai\n",
    "print(\"Modified - MAPE :\",MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute R^2 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 error:  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "df_5d = pd.read_csv('5_d.csv')\n",
    "n = len(df_5d)\n",
    "y_mean = 0\n",
    "ss_total = 0\n",
    "ss_res = 0\n",
    "y_mean =np.mean(df_5d['y'])\n",
    "for i in range(n):\n",
    "    ss_total += (df_5d['y'][i] - y_mean)**2\n",
    "    ss_res += (df_5d['y'][i] - df_5d['pred'][i])**2\n",
    "R_square = 1 -(ss_res/ss_total)\n",
    "\n",
    "print(\"R^2 error: \", R_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
